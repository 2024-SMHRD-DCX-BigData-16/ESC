{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… ìë™ ì„¤ì •ëœ ìˆ˜ì§‘ ê¸°ê°„: 20250128 ~ 20250428\n",
      "Total Count: 31502\n",
      "Collect Data Count: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_18840\\3262743490.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['image_url'] = image_urls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ CSV ì €ì¥ ì™„ë£Œ: C:\\Users\\SMHRD\\Desktop\\ì‹¤ì „\\ESC\\fastAPI\\crewling\\date\\auto_top10_í†µí•©_20250428.csv\n",
      "ğŸš€ ì„œë²„ ì‘ë‹µ: âœ… CSV ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤... (Ctrl+Cë¡œ ì¢…ë£Œ)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ğŸ”¹ ì €ì¥í•  ë””ë ‰í† ë¦¬\n",
    "SAVE_DIR = r\"C:\\Users\\SMHRD\\Desktop\\ì‹¤ì „\\ESC\\fastAPI\\crewling\\date\"\n",
    "\n",
    "# ğŸ”¹ ë‰´ìŠ¤ ë¶„ë¥˜ í•¨ìˆ˜ (ê³ ìš©/ì‚°ì¬)\n",
    "def assign_label(row):\n",
    "    content = (row['title'] or '') + ' ' + (row['content'] or '')\n",
    "    if any(keyword in content for keyword in ['ì‚°ì¬', 'ì‚°ì—…ì¬í•´', 'ì§ì—…ë³‘', 'ìš”ì–‘', 'ê³µìƒ', 'ì—…ë¬´ìƒì§ˆë³‘', 'ê·¼ê³¨ê²©ê³„']):\n",
    "        return 'ì‚°ì¬'\n",
    "    elif any(keyword in content for keyword in ['ê³ ìš©', 'ë…¸ë™', 'ê·¼ë¡œ', 'ì¼ìë¦¬', 'ì„ê¸ˆ', 'í•´ê³ ', 'ì²´ë¶ˆ', 'í”¼í•´', 'ê³„ì•½', 'ë¶€ë‹¹', 'ì‹ ê³ ']):\n",
    "        return 'ê³ ìš©'\n",
    "    else:\n",
    "        return 'ê¸°íƒ€'\n",
    "\n",
    "# ğŸ”¹ Top5 ë‰´ìŠ¤ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_top5(news_sample):\n",
    "    if news_sample.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(news_sample['content'])\n",
    "    kmeans = KMeans(n_clusters=min(20, len(news_sample)), random_state=42)\n",
    "    news_sample['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    closest_docs = []\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_indices = np.where(news_sample['cluster'] == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        cluster_vectors = X[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_vectors - centroids[i], axis=1)\n",
    "        closest_doc_index = cluster_indices[np.argmin(distances)]\n",
    "        closest_docs.append(closest_doc_index)\n",
    "\n",
    "    issue_top_df = news_sample.iloc[closest_docs]\n",
    "\n",
    "    titles = issue_top_df['title'].fillna('').tolist()\n",
    "    title_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = title_vectorizer.fit_transform(titles)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    similarity_score = cosine_sim.sum(axis=1)\n",
    "    top_indices = similarity_score.argsort()[::-1][:5]\n",
    "    auto_top5_df = issue_top_df.iloc[top_indices]\n",
    "\n",
    "    return auto_top5_df\n",
    "\n",
    "# ğŸ”¹ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_job():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"ğŸ“… ìë™ ì„¤ì •ëœ ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((ê³ ìš© || ê·¼ë¡œ || ë…¸ë™ || ì¼ìë¦¬) && (ì„ê¸ˆ || í•´ê³  || ì²´ë¶ˆ || í”¼í•´ || ê³„ì•½ || ë¶€ë‹¹ || ì‹ ê³ )) || (ì‚°ì¬ || ì‚°ì—…ì¬í•´ || ì§ì—…ë³‘ || ìš”ì–‘ || ê³µìƒ || ì—…ë¬´ìƒì§ˆë³‘ || ê·¼ê³¨ê²©ê³„)'\n",
    "    urlString = 'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'\n",
    "\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 2):  # í…ŒìŠ¤íŠ¸ìš© 1í˜ì´ì§€ (ì‹¤ì „ì€ 31ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"âš ï¸ í˜ì´ì§€ {pageNum} ì‘ë‹µì— 'item' ì—†ìŒ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í˜ì´ì§€ {pageNum} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Total Count:\", response.json()['item']['totalCnt'])\n",
    "    print(\"Collect Data Count:\", len(doc_list))\n",
    "\n",
    "    df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url', 'vks']]\n",
    "\n",
    "    # ğŸ”¹ ê³ ìš©/ì‚°ì¬ ìë™ ë¶„ë¥˜\n",
    "    df['label'] = df.apply(assign_label, axis=1)\n",
    "    df = df[df['label'] != 'ê¸°íƒ€']  # ê¸°íƒ€ ì œê±°\n",
    "\n",
    "    # ğŸ”¹ ê³ ìš©/ì‚°ì¬ ê°ê° ì¶”ì¶œ\n",
    "    employment_news = df[df['label'] == 'ê³ ìš©'].copy()\n",
    "    industrial_accident_news = df[df['label'] == 'ì‚°ì¬'].copy()\n",
    "\n",
    "    # ğŸ”¹ ìƒ˜í”Œë§\n",
    "    employment_news_sample = employment_news.sample(n=min(500, len(employment_news)), random_state=42)\n",
    "    industrial_accident_news_sample = industrial_accident_news.sample(n=min(500, len(industrial_accident_news)), random_state=42)\n",
    "\n",
    "    # ğŸ”¹ Top5 ì¶”ì¶œ\n",
    "    top5_employment = extract_top5(employment_news_sample)\n",
    "    top5_industrial = extract_top5(industrial_accident_news_sample)\n",
    "\n",
    "    # ğŸ”¹ ê³ ìš©+ì‚°ì¬ 5+5 í•©ì¹˜ê¸°\n",
    "    top10_df = pd.concat([top5_employment, top5_industrial], ignore_index=True)\n",
    "\n",
    "    # ğŸ”¹ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ\n",
    "    final_df = top10_df[['date', 'writerName', 'title', 'content', 'url']]\n",
    "\n",
    "    image_urls = []\n",
    "    for url in final_df['url']:\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            res = requests.get(url, headers=headers, timeout=10)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image_url = og_image['content']\n",
    "            else:\n",
    "                image_url = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ URL ì—ëŸ¬: {url} ({e})\")\n",
    "            image_url = None\n",
    "\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    final_df['image_url'] = image_urls\n",
    "\n",
    "    # ğŸ”¹ ìµœì¢… ì €ì¥\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_í†µí•©_{today_str}.csv\")\n",
    "    final_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"ğŸ“ CSV ì €ì¥ ì™„ë£Œ: {final_file_path}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "        \"http://localhost:8087/api/news/save-csv\",\n",
    "        json={\"filePath\": final_file_path}\n",
    "        )\n",
    "        print(f\"ğŸš€ ì„œë²„ ì‘ë‹µ: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Spring ì„œë²„ì— ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # ğŸ”¹ ì‹¤í–‰ + ìŠ¤ì¼€ì¤„ ì„¤ì •\n",
    "run_job()\n",
    "\n",
    "schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "print(\"ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤... (Ctrl+Cë¡œ ì¢…ë£Œ)\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ab266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
