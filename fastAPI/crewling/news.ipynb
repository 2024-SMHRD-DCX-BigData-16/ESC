{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f34c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 자동 설정된 수집 기간: 20250128 ~ 20250428\n",
      "Total Count: 31502\n",
      "Collect Data Count: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_18840\\3262743490.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['image_url'] = image_urls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 CSV 저장 완료: C:\\Users\\SMHRD\\Desktop\\실전\\ESC\\fastAPI\\crewling\\date\\auto_top10_통합_20250428.csv\n",
      "🚀 서버 응답: ✅ CSV 데이터가 저장되었습니다!\n",
      "스케줄러가 작동 중입니다... (Ctrl+C로 종료)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 173\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    172\u001b[39m     schedule.run_pending()\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 🔹 필요 라이브러리 import\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 🔹 저장할 디렉토리\n",
    "SAVE_DIR = r\"C:\\Users\\SMHRD\\Desktop\\실전\\ESC\\fastAPI\\crewling\\date\"\n",
    "\n",
    "# 🔹 뉴스 분류 함수 (고용/산재)\n",
    "def assign_label(row):\n",
    "    content = (row['title'] or '') + ' ' + (row['content'] or '')\n",
    "    if any(keyword in content for keyword in ['산재', '산업재해', '직업병', '요양', '공상', '업무상질병', '근골격계']):\n",
    "        return '산재'\n",
    "    elif any(keyword in content for keyword in ['고용', '노동', '근로', '일자리', '임금', '해고', '체불', '피해', '계약', '부당', '신고']):\n",
    "        return '고용'\n",
    "    else:\n",
    "        return '기타'\n",
    "\n",
    "# 🔹 Top5 뉴스 추출 함수\n",
    "def extract_top5(news_sample):\n",
    "    if news_sample.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(news_sample['content'])\n",
    "    kmeans = KMeans(n_clusters=min(20, len(news_sample)), random_state=42)\n",
    "    news_sample['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    closest_docs = []\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_indices = np.where(news_sample['cluster'] == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        cluster_vectors = X[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_vectors - centroids[i], axis=1)\n",
    "        closest_doc_index = cluster_indices[np.argmin(distances)]\n",
    "        closest_docs.append(closest_doc_index)\n",
    "\n",
    "    issue_top_df = news_sample.iloc[closest_docs]\n",
    "\n",
    "    titles = issue_top_df['title'].fillna('').tolist()\n",
    "    title_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = title_vectorizer.fit_transform(titles)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    similarity_score = cosine_sim.sum(axis=1)\n",
    "    top_indices = similarity_score.argsort()[::-1][:5]\n",
    "    auto_top5_df = issue_top_df.iloc[top_indices]\n",
    "\n",
    "    return auto_top5_df\n",
    "\n",
    "# 🔹 메인 실행 함수\n",
    "def run_job():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"📅 자동 설정된 수집 기간: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((고용 || 근로 || 노동 || 일자리) && (임금 || 해고 || 체불 || 피해 || 계약 || 부당 || 신고)) || (산재 || 산업재해 || 직업병 || 요양 || 공상 || 업무상질병 || 근골격계)'\n",
    "    urlString = '  '\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 2):  # 테스트용 1페이지 (실전은 31로 변경 가능)\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"⚠️ 페이지 {pageNum} 응답에 'item' 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 페이지 {pageNum} 수집 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Total Count:\", response.json()['item']['totalCnt'])\n",
    "    print(\"Collect Data Count:\", len(doc_list))\n",
    "\n",
    "    final_df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url']]\n",
    " # 🔹 이미지 URL 수집\n",
    "    image_urls = []\n",
    "    for url in final_df['url']:\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            res = requests.get(url, headers=headers, timeout=10)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image_url = og_image['content']\n",
    "            else:\n",
    "                image_url = None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ URL 에러: {url} ({e})\")\n",
    "            image_url = None\n",
    "\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    final_df['image_url'] = image_urls\n",
    "\n",
    "    # 🔹 최종 저장\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_통합_{today_str}.csv\")\n",
    "    final_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"📁 CSV 저장 완료: {final_file_path}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "        \"http://localhost:8087/api/news/save-csv\",\n",
    "        json={\"filePath\": final_file_path}\n",
    "        )\n",
    "        print(f\"🚀 서버 응답: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Spring 서버에 요청 실패: {e}\")\n",
    "\n",
    "    # 🔹 실행 + 스케줄 설정\n",
    "run_job()\n",
    "\n",
    "schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "print(\"스케줄러가 작동 중입니다... (Ctrl+C로 종료)\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966632d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import schedule\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "from kiwipiepy import Kiwi\n",
    "# from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# [수집] → [명사추출] → [토픽모델링] → [군집화] → [대표뉴스/키워드 요약](3개월) -> 여기부터 하루씩 다시 시작하자!!(하루 뉴스데이터)\n",
    "# → df로 나머지 정보 다시 불러오기 -> df의 유사도 다시 업데이트하기  → [Top10 추출] -> [저장/전송]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfe36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 수집 기간: 20250129 ~ 20250429\n",
      "총 수집 건수: 15000\n",
      "✅ cluster_summary_20250429.csv 저장 완료\n",
      "📁 CSV 저장 완료: ./date\\auto_top10_통합_20250429.csv\n",
      "🚀 서버 응답: ✅ CSV 데이터가 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 전역 설정\n",
    "# mecab = Mecab()\n",
    "kiwi = Kiwi()\n",
    "gtr_ymd = datetime.today().strftime('%y%m%d')  # 예: 250429\n",
    "SAVE_DIR = './date'\n",
    "\n",
    "def extract_nouns(text):\n",
    "    result = kiwi.analyze(text)\n",
    "    nouns = []\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if pos.startswith('N') and len(token) > 1:\n",
    "            nouns.append(token)\n",
    "    return nouns\n",
    "\n",
    "# 2. 뉴스 수집 함수\n",
    "def collect_news():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"📅 수집 기간: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((고용 || 근로 || 노동 || 일자리) && (임금 || 해고 || 체불 || 피해 || 계약 || 부당 || 신고)) || (산재 || 산업재해 || 직업병 || 요양 || 공상 || 업무상질병 || 근골격계)'\n",
    "    urlString = 'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'\n",
    "\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 31):  # 테스트는 1, 실전은 31\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"⚠️ 페이지 {pageNum} 응답에 'item' 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 페이지 {pageNum} 수집 실패: {e}\")\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"총 수집 건수:\", len(doc_list))\n",
    "\n",
    "    df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url', 'vks']]\n",
    "    df.fillna('', inplace=True)\n",
    "    df['sentence'] = df[['title', 'content']].apply(\" \".join, axis=1)\n",
    "    df['nouns'] = df['sentence'].apply(lambda x: ', '.join(extract_nouns(x)))\n",
    "\n",
    "    return df\n",
    "# 3. 토픽 모델링 함수\n",
    "def extract_lda_topic(df):\n",
    "    _df = df.copy() \n",
    "    corpus = df['nouns'].str.split(', ').tolist()\n",
    "\n",
    "    # 토픽 개수 설정\n",
    "    if len(corpus) >= 1000: k = 100\n",
    "    elif len(corpus) >= 500: k = 70\n",
    "    elif len(corpus) >= 200: k = 40\n",
    "    else: k = 20\n",
    "\n",
    "    \n",
    "    lda_model = tp.LDAModel(tw=tp.TermWeight.PMI, min_df=3, rm_top=0, k=k, seed=572)\n",
    "\n",
    "    for lis in corpus:\n",
    "        if lis != []:\n",
    "            lda_model.add_doc(lis)\n",
    "\n",
    "    for i in range(0, 200, 4):\n",
    "        lda_model.train(4, workers=1)\n",
    "\n",
    "    word_df = pd.DataFrame()\n",
    "    top_n = 10\n",
    "    for i in range(lda_model.k):\n",
    "        topic_words = [keyword for keyword, score in lda_model.get_topic_words(i, top_n=top_n)]\n",
    "        topic_scores = [score for keyword, score in lda_model.get_topic_words(i, top_n=top_n)]\n",
    "        topic_seq = list(range(len(topic_words)))\n",
    "        _word_df = pd.DataFrame(data={'word_seq': topic_seq, 'word': topic_words, 'pmi_score': topic_scores})\n",
    "        _word_df['topic_id'] = f'topic_{gtr_ymd}_{i}'\n",
    "        # _word_df['embassy_cd'] = _df['embassy_cd'].unique()[0]\n",
    "        _word_df = _word_df[['topic_id', 'word_seq', 'word', 'pmi_score']]\n",
    "        word_df = pd.concat([word_df, _word_df])\n",
    "\n",
    "    word_df = word_df.reset_index(drop=True)\n",
    "    word_df = word_df[['topic_id','word_seq','word','pmi_score']]\n",
    "    word_df['pmi_score'] = word_df.pmi_score.round(5)\n",
    "\n",
    "    topic_dist = [lda_model.infer((lda_model.docs[i]))[0] for i in range(len(_df))]\n",
    "\n",
    "    _df['max_topic_num'] = [vec.argmax() for vec in topic_dist]\n",
    "    _df['max_topic_value'] = [vec.max() for vec in topic_dist] \n",
    "\n",
    "    use_index = _df['max_topic_num'].value_counts().index.tolist()\n",
    "\n",
    "    topic_items = []\n",
    "    for i in use_index:\n",
    "        topic_dict = dict()\n",
    "        topic_words = [keyword for keyword, score in lda_model.get_topic_words(i, top_n=20) if score >= 0.01]\n",
    "        if len(topic_words) >= 5:\n",
    "            topic_dict['mdl_index'] = i\n",
    "            topic_dict['topic_words'] = ', '.join(topic_words[:5])\n",
    "            topic_items.append(topic_dict)\n",
    "\n",
    "    topic_df = pd.DataFrame(topic_items)\n",
    "    if len(topic_df) == 0:\n",
    "        return word_df, topic_df\n",
    "\n",
    "    union_dict = dict()\n",
    "    mdl_index_list = topic_df['mdl_index'].tolist()\n",
    "    mdl_index_list.reverse()\n",
    "    count = 0\n",
    "    for i in mdl_index_list[1:]:\n",
    "        count += 1\n",
    "        target_topic_words = topic_df['topic_words'][topic_df['mdl_index'] == i].tolist()[0].split(', ')\n",
    "\n",
    "        for j in topic_df['mdl_index'].tolist()[-count:]:\n",
    "            subject_topic_words = topic_df['topic_words'][topic_df['mdl_index'] == j].tolist()[0].split(', ')\n",
    "            union = set(target_topic_words) & set(subject_topic_words)\n",
    "            if len(union) >= 3:\n",
    "                union_dict[j] = i\n",
    "\n",
    "    _df['new_topic_num'] = _df['max_topic_num']\n",
    "    for old, new in union_dict.items():\n",
    "        _df.loc[_df['max_topic_num'] == old, 'new_topic_num'] = new\n",
    "\n",
    "    info_df = pd.DataFrame(data={'topic_num':range(lda_model.k)})\n",
    "\n",
    "    info_df['new_topic_num'] = info_df['topic_num']\n",
    "    for old, new in union_dict.items():\n",
    "        info_df.loc[info_df['topic_num'] == old, 'new_topic_num'] = new\n",
    "\n",
    "    count_df = _df['max_topic_num'].value_counts().reset_index()\n",
    "    count_df.columns = ['topic_num', 'doc_cnt']\n",
    "    info_df = info_df.merge(count_df, how='left').fillna(0)\n",
    "    info_df.doc_cnt = info_df.doc_cnt.astype('int')\n",
    "\n",
    "    _df = _df[_df['max_topic_value'] >= 0.5].reset_index(drop=True)\n",
    "\n",
    "    temp_df = _df['max_topic_num'].value_counts()\n",
    "    use_index = temp_df[temp_df >= 3].index.tolist()\n",
    "\n",
    "    _df = _df[_df['max_topic_num'].isin(use_index)].reset_index(drop=True)\n",
    "    _df = _df[_df['max_topic_num'].isin(topic_df['mdl_index'].tolist())].reset_index(drop=True)\n",
    "\n",
    "    topic_df = topic_df[topic_df['mdl_index'].isin(_df['new_topic_num'].unique().tolist())]\n",
    "\n",
    "    _df.reset_index(inplace=True)\n",
    "    _df.rename(columns={'index':'docid'}, inplace=True)\n",
    "\n",
    "    top_df = pd.DataFrame()\n",
    "    new_topic_number = 0\n",
    "    for topic_number in _df['new_topic_num'].value_counts().index:\n",
    "        temp_df = _df[_df['new_topic_num'] == topic_number].reset_index(drop=True)\n",
    "\n",
    "        corpus = temp_df['nouns'].tolist()\n",
    "        docid = temp_df['docid'].tolist()\n",
    "\n",
    "        # tf-idf 적용\n",
    "        tfidfv = TfidfVectorizer().fit(corpus)\n",
    "        vector = tfidfv.transform(corpus).toarray()\n",
    "\n",
    "        # k값 설정, 학습\n",
    "        km = KMeans(n_clusters = 1)\n",
    "        km.fit(vector)\n",
    "\n",
    "        # 클러스터 결과로 데이터프레임 재구축\n",
    "        results = []\n",
    "        clusters = km.labels_.tolist()  # 군집화 결과(라벨)\n",
    "        for i, value in enumerate(clusters):\n",
    "            \n",
    "            result_dict = {}\n",
    "            result_dict['mdl_index'] = topic_number\n",
    "            result_dict['title'] = temp_df.loc[temp_df.docid == docid[i]]['title'].tolist()[0]\n",
    "            result_dict['nouns'] = temp_df.loc[temp_df.docid == docid[i]]['nouns'].tolist()[0]\n",
    "            result_dict['topic_value'] = temp_df.loc[temp_df.docid == docid[i]]['max_topic_value'].tolist()[0]\n",
    "            vec1 = vector[i].reshape(1, -1)  # 뉴스 벡터\n",
    "            vec2 = km.cluster_centers_[int(value)].reshape(1,-1)  # k중앙값 벡터\n",
    "            result_dict['similarity'] = cosine_similarity(vec1, vec2)[0][0]  # 코사인 유사도 계산\n",
    "            results.append(result_dict)\n",
    "        \n",
    "        result_df = pd.DataFrame(results).sort_values(['mdl_index','similarity'], ascending=[True, False]).reset_index(drop=True)\n",
    "        top_df = pd.concat([top_df, result_df])\n",
    "\n",
    "    top_df = top_df[top_df['similarity'] >= 0.2].reset_index(drop=True)\n",
    "\n",
    "    topic_df['doc_count'] = 0\n",
    "    for mdl_index, doc_count in top_df['mdl_index'].value_counts().items():\n",
    "        topic_df.loc[topic_df['mdl_index'] == mdl_index, 'doc_count'] = doc_count\n",
    "\n",
    "    return top_df, topic_df, info_df, word_df\n",
    "\n",
    "# 4. 이슈 키워드 추출 \n",
    "def extract_issue_kwd(top_df, topic_df, info_df):\n",
    "    stopwords = [\n",
    "    '통보', '계획', '송부', '보고', '회신', '결과', '제출', '참석', '공지', '안내', '요청', '접수', '확인', \n",
    "    '조치', '진행', '관리', '공고', '변경', '개정', '신청', '접수', '확대', '축소', '개선', '이행',\n",
    "    '회의', '보고서', '문서', '서류', '자료', '파일', '작성', '배포', '확산', '사업', '전달',\n",
    "    '월', '주년', '분기', '기간', '시행', '오전', '오후', '조사', '평가', '진단', '대책', '방안',\n",
    "    '제도', '정책', '지원', '프로그램', '참여'\n",
    "]\n",
    "    main_keywords = []\n",
    "    for mdl_index in top_df['mdl_index'].value_counts().index:\n",
    "        main_keyword_dict = dict()\n",
    "\n",
    "        # 토픽단어 중 보편적인 단어(stopwords) 제거\n",
    "        try:\n",
    "            re_ngram = ', '.join(topic_df['topic_words'][topic_df['mdl_index'] == mdl_index]).split(', ')\n",
    "            re_ngram = [x for x in re_ngram if x not in stopwords]\n",
    "        except:\n",
    "            break\n",
    "        # 제목에서 ()괄호 삭제\n",
    "        morph_title = top_df[top_df['mdl_index'] == mdl_index].title.str.replace(r'\\(\\w.*\\)', '', regex=True).tolist()\n",
    "\n",
    "        keyword_list = []\n",
    "        for title in morph_title:\n",
    "            try:\n",
    "                # 제목에서 명사만 남김(title)\n",
    "                query = ' '.join(extract_nouns(title))\n",
    "\n",
    "                # 토픽단어 top1 1개로 쿼리에서 6자 이상 단어 조회\n",
    "                use_word = r'\\w*{}\\w*'.format(re_ngram[0])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if keywords != []:\n",
    "                    keyword = keywords[0]\n",
    "                    if len(keyword) > 5:\n",
    "                        keyword_list.append(keyword)\n",
    "\n",
    "                # 토픽단어 top1~2 2개로 쿼리에서 단어 조회\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "\n",
    "                # 토픽단어 top2 1개로 쿼리에서 6자 이상 단어 조회\n",
    "                use_word = r'\\w*{}\\w*'.format(re_ngram[1])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if keywords != []:\n",
    "                    keyword = keywords[0]\n",
    "                    if len(keyword) > 5:\n",
    "                        keyword_list.append(keyword)\n",
    "                        continue\n",
    "\n",
    "                # 토픽단어 top1~3, 3개로 쿼리에서 단어 조회\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1], re_ngram[2])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "\n",
    "                # 토픽단어 top1~3 2개로 제목에서 단어 조회\n",
    "                use_word = r'\\w*{}\\w*|\\w*{}\\w*|\\w*{}\\w*'.format(re_ngram[0], re_ngram[1], re_ngram[2])\n",
    "                keywords = re.findall(use_word, query)\n",
    "                if len(keywords) == 2:\n",
    "                    keyword = ' '.join(keywords)\n",
    "                    keyword_list.append(keyword)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        try:\n",
    "            main_kwd = max([kwd for kwd, cnt in Counter(keyword_list).items() if (cnt == max(Counter(keyword_list).values())) & (cnt >= 2)], key=len)\n",
    "            main_keyword_dict['mdl_index'] = mdl_index\n",
    "            main_keyword_dict['main_kwd'] = main_kwd\n",
    "            main_keywords.append(main_keyword_dict)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    issue_df = pd.DataFrame(main_keywords)\n",
    "    issue_df.columns = ['new_topic_num', 'iss_kwd']\n",
    "    issue_df = issue_df.merge(info_df.groupby(['new_topic_num'])['doc_cnt'].sum().reset_index(), how='left')\n",
    "    issue_df = issue_df.sort_values(['doc_cnt', 'iss_kwd'], ascending=[False, True]).reset_index(drop=True)\n",
    "    issue_df['topic_id'] = f'topic_{gtr_ymd}_' + issue_df.new_topic_num.astype('string')\n",
    "    issue_df['use_yn'] = 'Y'\n",
    "    issue_df = issue_df[~issue_df.iss_kwd.duplicated()].reset_index(drop=True)\n",
    "    issue_df['kwd_rank'] = issue_df.index + 1\n",
    "    issue_df = issue_df[['topic_id', 'kwd_rank', 'iss_kwd', 'doc_cnt', 'use_yn']]\n",
    "    issue_df.rename(columns={'iss_kwd':'topic_iss_kwd'}, inplace=True)\n",
    "\n",
    "    return issue_df\n",
    "\n",
    "# 5. 대표 뉴스 및 요약 \n",
    "def summarize_clusters(top_df):\n",
    "    cluster_summary = []\n",
    "\n",
    "    for mdl_index in sorted(top_df['mdl_index'].unique()):\n",
    "        cluster_info = {}\n",
    "\n",
    "        # 해당 군집 데이터\n",
    "        cluster_df = top_df[top_df['mdl_index'] == mdl_index]\n",
    "\n",
    "        # 1. 대표 뉴스 (similarity 가장 높은 제목)\n",
    "        best_news = cluster_df.sort_values(by='similarity', ascending=False).iloc[0]\n",
    "        cluster_info['mdl_index'] = mdl_index\n",
    "        cluster_info['대표뉴스제목'] = best_news['title']\n",
    "\n",
    "        # 2. 군집 내 명사 모아서 핵심 키워드 5개 추출\n",
    "        all_nouns = []\n",
    "        for nouns in cluster_df['nouns']:\n",
    "            all_nouns += nouns.split(', ')\n",
    "        noun_counts = Counter(all_nouns)\n",
    "        top_nouns = [noun for noun, count in noun_counts.most_common(5)]\n",
    "        cluster_info['대표키워드'] = ', '.join(top_nouns)\n",
    "\n",
    "        # 3. 군집 평균 similarity\n",
    "        avg_similarity = cluster_df['similarity'].mean()\n",
    "        cluster_info['군집평균유사도'] = round(avg_similarity, 4)\n",
    "\n",
    "        # 4. 뉴스 개수 (doc 수)\n",
    "        cluster_info['뉴스개수'] = len(cluster_df)\n",
    "\n",
    "        cluster_summary.append(cluster_info)\n",
    "\n",
    "    # 전체 군집 요약 데이터프레임\n",
    "    issue_summary_df = pd.DataFrame(cluster_summary)\n",
    "\n",
    "    # 🔥 여기서 Top10만 추출\n",
    "    issue_summary_df = issue_summary_df.sort_values(\n",
    "        by=['뉴스개수', '군집평균유사도'], \n",
    "        ascending=[False, False]\n",
    "    ).head(10).reset_index(drop=True)\n",
    "\n",
    "    return issue_summary_df\n",
    "\n",
    "# 6. 저장 및 전송\n",
    "def save_and_send(top_df):\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_통합_{today_str}.csv\")\n",
    "    top_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"📁 CSV 저장 완료: {final_file_path}\")\n",
    "\n",
    "    # try:\n",
    "    #     response = requests.post(\n",
    "    #         \"http://localhost:8087/api/news/save-csv\",\n",
    "    #         json={\"filePath\": final_file_path}\n",
    "    #     )\n",
    "    #     print(f\"🚀 서버 응답: {response.text}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ 서버 요청 실패: {e}\")\n",
    "\n",
    "# 7. 메인 실행 \n",
    "def main():\n",
    "    df = collect_news()\n",
    "    _df = df.copy()\n",
    "    top_df, topic_df, info_df, word_df = extract_lda_topic(_df)\n",
    "    issue_summary_df = summarize_clusters(top_df)\n",
    "\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    issue_summary_df.to_csv(f'./date/cluster_summary_{today_str}.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ cluster_summary_{today_str}.csv 저장 완료\")\n",
    "    save_and_send(top_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# # 8. 스케줄러\n",
    "# run_job()  # 처음 1번 실행\n",
    "\n",
    "# schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "# print(\"스케줄러 작동 중입니다... (Ctrl+C로 종료)\")\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
