{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 자동 설정된 수집 기간: 20250128 ~ 20250428\n",
      "Total Count: 31502\n",
      "Collect Data Count: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_18840\\3262743490.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['image_url'] = image_urls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 CSV 저장 완료: C:\\Users\\SMHRD\\Desktop\\실전\\ESC\\fastAPI\\crewling\\date\\auto_top10_통합_20250428.csv\n",
      "🚀 서버 응답: ✅ CSV 데이터가 저장되었습니다!\n",
      "스케줄러가 작동 중입니다... (Ctrl+C로 종료)\n"
     ]
    }
   ],
   "source": [
    "# 🔹 필요 라이브러리 import\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 🔹 저장할 디렉토리\n",
    "SAVE_DIR = r\"C:\\Users\\SMHRD\\Desktop\\실전\\ESC\\fastAPI\\crewling\\date\"\n",
    "\n",
    "# 🔹 뉴스 분류 함수 (고용/산재)\n",
    "def assign_label(row):\n",
    "    content = (row['title'] or '') + ' ' + (row['content'] or '')\n",
    "    if any(keyword in content for keyword in ['산재', '산업재해', '직업병', '요양', '공상', '업무상질병', '근골격계']):\n",
    "        return '산재'\n",
    "    elif any(keyword in content for keyword in ['고용', '노동', '근로', '일자리', '임금', '해고', '체불', '피해', '계약', '부당', '신고']):\n",
    "        return '고용'\n",
    "    else:\n",
    "        return '기타'\n",
    "\n",
    "# 🔹 Top5 뉴스 추출 함수\n",
    "def extract_top5(news_sample):\n",
    "    if news_sample.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(news_sample['content'])\n",
    "    kmeans = KMeans(n_clusters=min(20, len(news_sample)), random_state=42)\n",
    "    news_sample['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    closest_docs = []\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_indices = np.where(news_sample['cluster'] == i)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        cluster_vectors = X[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_vectors - centroids[i], axis=1)\n",
    "        closest_doc_index = cluster_indices[np.argmin(distances)]\n",
    "        closest_docs.append(closest_doc_index)\n",
    "\n",
    "    issue_top_df = news_sample.iloc[closest_docs]\n",
    "\n",
    "    titles = issue_top_df['title'].fillna('').tolist()\n",
    "    title_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = title_vectorizer.fit_transform(titles)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    similarity_score = cosine_sim.sum(axis=1)\n",
    "    top_indices = similarity_score.argsort()[::-1][:5]\n",
    "    auto_top5_df = issue_top_df.iloc[top_indices]\n",
    "\n",
    "    return auto_top5_df\n",
    "\n",
    "# 🔹 메인 실행 함수\n",
    "def run_job():\n",
    "    today = datetime.today()\n",
    "    three_months_ago = today - timedelta(days=90)\n",
    "    start_date = three_months_ago.strftime('%Y%m%d')\n",
    "    end_date = today.strftime('%Y%m%d')\n",
    "\n",
    "    print(f\"📅 자동 설정된 수집 기간: {start_date} ~ {end_date}\")\n",
    "\n",
    "    keyword = '((고용 || 근로 || 노동 || 일자리) && (임금 || 해고 || 체불 || 피해 || 계약 || 부당 || 신고)) || (산재 || 산업재해 || 직업병 || 요양 || 공상 || 업무상질병 || 근골격계)'\n",
    "    urlString = 'http://qt.some.co.kr/TrendMap/JSON/ServiceHandler'\n",
    "\n",
    "    doc_list = []\n",
    "    for pageNum in range(1, 2):  # 테스트용 1페이지 (실전은 31로 변경 가능)\n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'source': 'news ',\n",
    "            'lang': 'ko',\n",
    "            'rowPerPage': '500',\n",
    "            'pageNum': pageNum,\n",
    "            'orderType': '1',\n",
    "            'command': 'GetKeywordDocuments'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(urlString, data=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if 'item' in response.json():\n",
    "                doc_list += response.json()['item']['documentList']\n",
    "            else:\n",
    "                print(f\"⚠️ 페이지 {pageNum} 응답에 'item' 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 페이지 {pageNum} 수집 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Total Count:\", response.json()['item']['totalCnt'])\n",
    "    print(\"Collect Data Count:\", len(doc_list))\n",
    "\n",
    "    df = pd.DataFrame(doc_list)[['date', 'writerName', 'title', 'content', 'url', 'vks']]\n",
    "\n",
    "    # 🔹 고용/산재 자동 분류\n",
    "    df['label'] = df.apply(assign_label, axis=1)\n",
    "    df = df[df['label'] != '기타']  # 기타 제거\n",
    "\n",
    "    # 🔹 고용/산재 각각 추출\n",
    "    employment_news = df[df['label'] == '고용'].copy()\n",
    "    industrial_accident_news = df[df['label'] == '산재'].copy()\n",
    "\n",
    "    # 🔹 샘플링\n",
    "    employment_news_sample = employment_news.sample(n=min(500, len(employment_news)), random_state=42)\n",
    "    industrial_accident_news_sample = industrial_accident_news.sample(n=min(500, len(industrial_accident_news)), random_state=42)\n",
    "\n",
    "    # 🔹 Top5 추출\n",
    "    top5_employment = extract_top5(employment_news_sample)\n",
    "    top5_industrial = extract_top5(industrial_accident_news_sample)\n",
    "\n",
    "    # 🔹 고용+산재 5+5 합치기\n",
    "    top10_df = pd.concat([top5_employment, top5_industrial], ignore_index=True)\n",
    "\n",
    "    # 🔹 필요한 컬럼만\n",
    "    final_df = top10_df[['date', 'writerName', 'title', 'content', 'url']]\n",
    "\n",
    "    image_urls = []\n",
    "    for url in final_df['url']:\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            res = requests.get(url, headers=headers, timeout=10)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            og_image = soup.find('meta', property='og:image')\n",
    "            if og_image and og_image.get('content'):\n",
    "                image_url = og_image['content']\n",
    "            else:\n",
    "                image_url = None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ URL 에러: {url} ({e})\")\n",
    "            image_url = None\n",
    "\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    final_df['image_url'] = image_urls\n",
    "\n",
    "    # 🔹 최종 저장\n",
    "    today_str = datetime.today().strftime('%Y%m%d')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    final_file_path = os.path.join(SAVE_DIR, f\"auto_top10_통합_{today_str}.csv\")\n",
    "    final_df.to_csv(final_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"📁 CSV 저장 완료: {final_file_path}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "        \"http://localhost:8087/api/news/save-csv\",\n",
    "        json={\"filePath\": final_file_path}\n",
    "        )\n",
    "        print(f\"🚀 서버 응답: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Spring 서버에 요청 실패: {e}\")\n",
    "\n",
    "    # 🔹 실행 + 스케줄 설정\n",
    "run_job()\n",
    "\n",
    "schedule.every().day.at(\"09:00\").do(run_job)\n",
    "\n",
    "print(\"스케줄러가 작동 중입니다... (Ctrl+C로 종료)\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ab266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
